{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/secretorange/ml-recommender/blob/main/MatrixFactorization.ipynb)\n",
        "\n",
        "This is a bare bones example of using Matrix Factorisation to build a recommendation system using pytorch.\n",
        "\n",
        "Matrix factorization is a popular technique used in recommender systems to predict user preferences. Imagine a big grid where rows are users, columns are items (like movies or products), and the cells contain ratings. Many cells in this grid are empty because not every user has rated every item. Matrix factorization works by breaking this large table into two smaller tables: one representing users and the other representing items. These smaller tables capture the underlying **factors** that influence user preferences, like genre preferences in movies. By multiplying these smaller tables, we can estimate the missing ratings in the original table, helping us recommend items that users are likely to enjoy."
      ],
      "metadata": {
        "id": "QjvN4cj062Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "xJlenTwHk45B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNdXh_OPgt9s"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import pdb;\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "class MovieDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    # Download the popular movielens dataset\n",
        "    ! curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o ml-latest-small.zip\n",
        "\n",
        "    with zipfile.ZipFile('ml-latest-small.zip', 'r') as zip:\n",
        "      zip.extractall('data')\n",
        "\n",
        "    movies_df = pd.read_csv('data/ml-latest-small/movies.csv')\n",
        "    ratings_df = pd.read_csv('data/ml-latest-small/ratings.csv')\n",
        "\n",
        "    # Label Encode the ids (the encodings will then match the indexes of the embeddings)\n",
        "    self.d = defaultdict(LabelEncoder)\n",
        "    for c in ['userId', 'movieId']:\n",
        "      # Encode the ids\n",
        "      self.d[c].fit(ratings_df[c].unique())\n",
        "\n",
        "      # Swap out the ids for the encoded values\n",
        "      ratings_df[c] = self.d[c].transform(ratings_df[c])\n",
        "\n",
        "\n",
        "    self.x = ratings_df.drop(['rating', 'timestamp'], axis=1).values\n",
        "    self.y = ratings_df['rating'].values\n",
        "    self.x, self.y = torch.tensor(self.x), torch.tensor(self.y)\n",
        "\n",
        "    users = ratings_df.userId.unique()\n",
        "    movies = ratings_df.movieId.unique()\n",
        "\n",
        "    self.n_users = len(users)\n",
        "    self.n_items = len(movies)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return (self.x[index], self.y[index])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Factorization"
      ],
      "metadata": {
        "id": "WpAISPhRk0wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class MatrixFactorization(torch.nn.Module):\n",
        "  def __init__(self, n_users, n_items, n_factors=20):\n",
        "    super().__init__()\n",
        "    # Create the embeddings that will be trained\n",
        "    self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
        "    self.item_factors = torch.nn.Embedding(n_items, n_factors)\n",
        "\n",
        "    # Initialise to random weights\n",
        "    self.user_factors.weight.data.uniform_(0, 0.05)\n",
        "    self.item_factors.weight.data.uniform_(0, 0.05)\n",
        "\n",
        "  def forward(self, data):\n",
        "    users, items = data[:,0], data[:, 1]\n",
        "\n",
        "    user_embedding = self.user_factors(users)\n",
        "    item_embedding = self.item_factors(items)\n",
        "\n",
        "    dot_product = (user_embedding * item_embedding).sum(1)\n",
        "\n",
        "    return dot_product\n",
        "\n",
        "  def predict(self, user, item):\n",
        "    data = torch.tensor([[user, item]], dtype=torch.long)\n",
        "    return self.forward(data)\n"
      ],
      "metadata": {
        "id": "3bVhjWr8iybE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test Split"
      ],
      "metadata": {
        "id": "w-faDsul7RwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set = MovieDataset()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(0.8 * len(train_set))\n",
        "test_size = len(train_set) - train_size\n",
        "train_dataset, test_dataset = random_split(train_set, [train_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Define the loss function, model and optimizer\n",
        "epochs = 128\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "model = MatrixFactorization(train_set.n_users, train_set.n_items, n_factors=8)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = loss_fn(outputs, y.type(torch.float32))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for the epoch\n",
        "    epoch_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Print the training loss for this epoch\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            predictions = model(x)\n",
        "            loss = loss_fn(predictions, y.type(torch.float32))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss = evaluate(model, test_loader)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "id": "mgg1ZWIPmO52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "yyixk4cQ7VJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(userId, movieId):\n",
        "  movieIndex = train_set.d['movieId'].transform([movieId])[0]\n",
        "  userIndex = train_set.d['userId'].transform([userId])[0]\n",
        "  predicted_rating = model.predict(userId, movieIndex)\n",
        "  print(f\"Predicted rating for user {userId} and item {movieId}: {predicted_rating}\")\n",
        "\n",
        "predict(1, 1)\n",
        "predict(1, 2)"
      ],
      "metadata": {
        "id": "hhU0toZ_7eNn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}